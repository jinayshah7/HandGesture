{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "I've chosen this as my final year project, I plan to complete it before February. This thread contains all the information you need to know about the project development so far, I hope itâ€™s helpful. I haven't started any of the coding part, currently I'm just surfing the web and gathering information before diving in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xbox Kinect, PlayStation Move, Leap Motion Controller all of them do the job we're aiming to complete, in fact they can perform better than the best model we'll be able to come up with. So, why aren't they so popular? I think it's because of their ridiculous cost. All of them use specialized cameras, sensors, lights, markers, and remotes to do the job. But what if we could do the same job with just a simple camera? After all, we humans can do it better than them using just our vision, no other telepathic sensors. Why can't the machines do it?\n",
    "\n",
    "Why can't we train a neural network to recognize gestures using just a simple 2D webcam? There are a couple of complications here, I'll explain them here. We need to account for different lighting conditions that the system will need to work in. Different people will have different way of performing the same gestures, they won't be identical. And the biggest bottleneck here is that we need to actually have enough training data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we'll be able to use Machine Learning to address most of the problems like lighting conditions and variations in the way people do the gestures, the main hurdle here is that we'll need a proper dataset to work with. This is where we're extremely lucky. A Germany based company has attempted to design and train this type of neural network. A developer of their company gave a presentation on the problems they encountered along the way and the way in which they tackled them. It's worth watching, if you want to gain a deeper insight of this problem(and the solution).\n",
    "\n",
    "https://www.youtube.com/watch?v=keffWSqi67w\n",
    "\n",
    "https://medium.com/twentybn/gesture-recognition-using-end-to-end-learning-from-a-large-video-database-2ecbfb4659ff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't seen the video, they paid people to perform gestures for them and eventually gathered a giant collection of 150k video clips of 25 gestures! The best part is that made the dataset publicly available for research purposes. We're going to use that dataset for our building our model. Check out the dataset here, it's publicly available to everyone as long as we only use it for research.\n",
    "\n",
    "https://20bn.com/datasets/jester/v1\n",
    "\n",
    "Here's another guy who tried different approaches for video classification and has documented brilliantly. Make sure you read all the three articles he wrote on the topic.\n",
    "\n",
    "https://blog.coast.ai/five-video-classification-methods-implemented-in-keras-and-tensorflow-99cad29cc0b5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D CNNs\n",
    "\n",
    "In their video, they(twentyBN) decribed quite few techinques which they went through but finally settled on a 3D CNN architecture.\n",
    "\n",
    "3D CNNs are not so good when it comes to performance, we need to find alternative ways to make this work out. 3D CNN is like a normal CNN but not only does it move over the x,y axis, it also moves over the time axis. Like a normal CNN would move over a flat frame, imagine stacking a dozen frames on top of each other and getting a cuboid of pixels. 3D CNN moves over such cuboids instead just a flat surface. I hope you get my point, I have little to explain without a proper animation.\n",
    "\n",
    "A part of the dataset contains videos with no labels, we can submit our predictions on those videos to get a ranking on their leaderboard. So far, a lot of researchers have successfully beat TwentyBN's 82% accuracy. I think we should try exploring their methods and rely on 3D CNNs as our last resort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motion Flow Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the best submissions in the leaderboard seem to be using this method to achieve their good scores. Instead of running a CNN on a single frame, they attach a couple of more frames to the original frame. These additional frames are made using the earlier frames and contain data about the change in motion, they're called motion flow frames.\n",
    "\n",
    "Imagine taking two frames from a video and calculating the gradient of those two, the parts where the difference between them is more are brighter and there's black color where there is no difference. This is how I've understood it and I'm sure it's an oversimplified explanation but it's close to what actually happens under the hood. The exact process is called calculating the \"optical flow\" between two frames. I've been warned that calculating the optical flow requires a lot of computing power, so this will be a complication. I'm not sure how fast OpenCV can calculate the optical flow of the frames but I came across a Python library which claims to do the same job really fast, it's based on C++. Here's the Github link\n",
    "\n",
    "https://github.com/pathak22/pyflow\n",
    "\n",
    "We'll see which one performs better among this and OpenCV's version and choose the one which is faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of researchers seem to be outperforming TwentyBN, not of them are providing their code. However, the guy who ranked second on the leaderboard has provided the code for his implementation, however it's in PyTorch. Here's the Github link\n",
    "\n",
    "https://github.com/okankop/MFF-pytorch\n",
    "\n",
    "He used Motion Flow Frames to get the performance he got, if anyone is aware of PyTorch and is able to explain the code, it  would be a great starting point for our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are others on the leaderboard. 1,4,5,7,10 are anonymous submissions with little to no explanation of what they used, I already googled that. What's DRX3D? It's mentioned in this paper on Motion Fused frames https://arxiv.org/pdf/1804.07187.pdf . There are other submissions which are almost similar to MFF. MFF and Spatio Temporal Networks are essentially the same thing.\n",
    "\n",
    "We're not only worried about getting a good accuracy, we're also concerned about getting real time performance. One thing we need to keep in mind is that we won't work with world class hardware like these guys in the top 10, we'll be working with just our normal laptop with our normal processor and in most cases, mediocre graphic card. The author of that medium article I linked above was talking about Raspberry Pi taking 4-5 seconds to make one prediction on the Inception Network, that's bad news. We'll also have to worry about practicality of the whole system along with the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
